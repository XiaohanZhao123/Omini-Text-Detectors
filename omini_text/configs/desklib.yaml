# Desklib AI Text Detector Configuration (v1.01)
# Supervised detection using custom transformer with mean pooling

# Model selection
model: desklib

# =============================================================================
# Model Configuration
# =============================================================================
# Path to Desklib model directory
# Default: baseline/desklib/ai-text-detector-v1.01
# Note: Model must be downloaded/available at this path
model_path: baseline/desklib/ai-text-detector-v1.01

# =============================================================================
# Device Configuration
# =============================================================================
# Device to use for inference
# Options: auto, cuda, cpu
# - auto: Automatically use GPU if available, otherwise CPU
# - cuda: Force GPU usage (requires CUDA-capable GPU)
# - cpu: Force CPU usage
# Recommendation: auto (optimal for most cases)
device: auto

# =============================================================================
# Detection Configuration
# =============================================================================
# Classification threshold
# Range: 0.0-1.0
# Texts with score >= threshold are classified as AI-generated (label=1)
# Trade-off: Higher threshold = more precision, lower recall
# Recommendation: 0.5 for balanced precision/recall
threshold: 0.5

# Maximum sequence length for tokenization
# Range: 128-1024
# Longer sequences require more memory but preserve full context
# Default: 768 (matches training configuration)
max_length: 768

# =============================================================================
# Performance Notes
# =============================================================================
# - Desklib uses custom transformer architecture with mean pooling
# - Can run on CPU or GPU (GPU recommended for faster inference)
# - Inference time: ~100ms per text on CPU, ~20ms on GPU
# - Maximum sequence length: Configurable (default 768)
# - Memory usage: ~500MB on GPU, ~300MB on CPU
# - Single text processing (batch processing not optimized)

# =============================================================================
# Model Architecture
# =============================================================================
# Base: Custom PreTrainedModel
# Components:
#   - Base transformer encoder (AutoModel)
#   - Mean pooling layer (attention-mask weighted averaging)
#   - Single-neuron classifier head (Linear layer)
#   - Sigmoid activation for probability output
# Loss Function: BCEWithLogitsLoss
# Output: Probability of text being AI-generated (0.0-1.0)

# =============================================================================
# Accuracy Benchmarks (from Desklib v1.01)
# =============================================================================
# Note: Exact benchmark results not publicly available
# Estimated performance based on architecture:
# - General accuracy: ~85-90% (estimated)
# - Suitable for: Basic AI text detection tasks
# - Comparison: Lower than e5-small (93.9%) but simpler architecture
#
# The model provides a baseline supervised approach with:
# - Simple, interpretable architecture
# - Easy to modify and retrain
# - Threshold-tunable for precision/recall trade-offs

# =============================================================================
# Training Details
# =============================================================================
# Version: v1.01
# Training Data: Custom dataset (details not publicly disclosed)
# Base Transformer: Configuration-based (loaded from model directory)
# Classifier: Single-layer MLP (hidden_size → 1)
# Pooling Strategy: Mean pooling with attention mask weighting
# Output: Binary classification (human vs AI-generated)

# =============================================================================
# Use Cases
# =============================================================================
# ✅ Recommended for:
# - Simple baseline AI text detection
# - Projects requiring model interpretability
# - Custom training and fine-tuning experiments
# - Educational purposes (understanding detection architecture)
# - Threshold tuning for specific precision/recall requirements
#
# ⚠️ Limitations:
# - Lower accuracy compared to state-of-the-art models
# - Supervised method: May not generalize to unseen AI models
# - Single text processing (not optimized for batches)
# - Limited public benchmarking data
# - Text length: Best performance on texts >50 tokens

# =============================================================================
# Comparison with Other Detectors
# =============================================================================
# vs E5-Small LoRA:
#   - Simpler architecture, easier to understand
#   - Lower accuracy (~85% vs 93.9%)
#   - Faster to retrain on custom data
#   - More interpretable (single classifier head)
#
# vs Fast-DetectGPT:
#   - Supervised vs zero-shot approach
#   - Faster inference (no perturbations needed)
#   - Requires training data
#   - Works on CPU without heavy GPU requirements
#
# vs Glimpse:
#   - Local model vs API-based
#   - No API costs
#   - Cannot detect proprietary models (GPT-4, Claude)
#   - Faster and more private (no external API calls)

# =============================================================================
# Customization Options
# =============================================================================
# Threshold Tuning:
# - threshold: 0.3 → High recall, low precision (detect more AI, more false positives)
# - threshold: 0.5 → Balanced (default recommendation)
# - threshold: 0.7 → High precision, low recall (fewer false positives, miss some AI)
#
# Max Length Tuning:
# - max_length: 512 → Faster, less memory, truncates long texts
# - max_length: 768 → Default, good balance
# - max_length: 1024 → Slower, more memory, preserves full context
#
# Device Selection:
# - device: cpu → Universal, slower, no GPU required
# - device: cuda → Faster, requires NVIDIA GPU with CUDA
# - device: auto → Recommended, adapts to available hardware

# =============================================================================
# Citation
# =============================================================================
# Model: Desklib AI Text Detector v1.01
# Source: baseline/desklib/ai-text-detector-v1.01
# Architecture: Custom transformer with mean pooling
# License: Check model directory for license information
